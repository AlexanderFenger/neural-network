# header

import os

# global settings
dir_main = os.getcwd()
# used files
root1 = dir_main + "/data/" + "OptimisedGeometry_BP0mm_2e10protons.root"
root2 = dir_main + "/data/" + "base_100ep_optimized0mm_training_pred.root"
root3 = dir_main + "/data/" + "base_100ep_optimized0mm_test_pred.root"
root4 = dir_main + "/data/" + "OptimisedGeometry_BP5mm_4e9protons.root"
root5 = dir_main + "/data/" + "base_100ep_optimized5mm_training_pred.root"
root6 = dir_main + "/data/" + "base_100ep_optimized5mm_test_pred.root"


#################################################################################################################

def generate_npz_resultsMC(root_mc, root_nn_training, root_nn_test, npz_filename):
    """
    extract monte carlo information from root, add the NN event selection tag extracted from Awals NN predictions

    root_mc: root file containing MC-information
    root_NN: root file generated by Awals Neural Network to export NN predictions from

    return: parent_root with added root leave
    """

    import numpy as np
    import uproot
    from sificc_lib.root_files import root_files
    from sificc_lib_awal.Simulation import Simulation

    # define dataframe header
    df_header = ["EventNumber",
                 "MCSimulatedEventType",
                 "CBIdentified",
                 "NNIdentified",
                 "MCEnergy_e",
                 "MCEnergy_p",
                 "MCPosition_source.x",
                 "MCPosition_source.y",
                 "MCPosition_source.z",
                 "MCDirection_source.x",
                 "MCDirection_source.y",
                 "MCDirection_source.z",
                 "MCComptenPosition.x",
                 "MCComptenPosition.y",
                 "MCComptenPosition.z",
                 "MCDirection_Scatter.x",
                 "MCDirection_Scatter.y",
                 "MCDirection_Scatter.z",
                 "MCPosition_e.x",
                 "MCPosition_e.y",
                 "MCPosition_e.z",
                 "MCPosition_p.x",
                 "MCPosition_p.y",
                 "MCPosition_p.z",
                 "IdealComptonEvent",
                 "origin_set",
                 "MCEnergyPrimary"]

    # create RootData object
    root_mc_data = Simulation(root_mc)

    # create dataframe
    df = np.zeros(shape=(root_mc_data.num_entries, len(df_header)))

    for i, event in enumerate(root_mc_data.iterate_events()):
        # define empty event data
        df_row = np.zeros(shape=(1, len(df_header)))
        # grab event data
        df_row[0, :] = [event.EventNumber,
                        event.event_type,
                        event.identification_code,
                        -1,  # add minus one to mark events not predicted by neural network
                        event.real_e_energy,
                        event.real_p_energy,
                        event.real_src_pos.x,
                        event.real_src_pos.y,
                        event.real_src_pos.z,
                        event.real_src_dir.x,
                        event.real_src_dir.y,
                        event.real_src_dir.z,
                        event.real_compton_pos.x,
                        event.real_compton_pos.y,
                        event.real_compton_pos.z,
                        event.real_scatter_dir.x,
                        event.real_scatter_dir.y,
                        event.real_scatter_dir.z,
                        event.real_e_position.x,
                        event.real_e_position.y,
                        event.real_e_position.z,
                        event.real_p_position.x,
                        event.real_p_position.y,
                        event.real_p_position.z,
                        event.is_ideal_compton,
                        0,
                        event.real_primary_energy]

        # write event data into dataframe
        df[i, :] = df_row

    # add the NN identified tag for the neural network predictions
    # open to extract NN training predictions and event number
    root_nn_data = uproot.open(root_nn_training)
    root_nn_data_tree = root_nn_data[b"ConeList"]

    # grab entries from leaves "GlobalEventNumber" and "EventType" and store them in arrays
    ary_GlobalEventNumber = root_nn_data_tree["GlobalEventNumber"].array()
    ary_EventType = root_nn_data_tree["EventType"].array()

    # sort both arrays by GlobalEventNumber
    ary_EventType = ary_EventType[ary_GlobalEventNumber.argsort()]
    ary_GlobalEventNumber = ary_GlobalEventNumber[ary_GlobalEventNumber.argsort()]

    # iterate the dataframe and scan for matching entries
    # an extremely dumb iteration algorithm but it works
    cidx = 0
    for i in range(df.shape[0]):
        # start at first entry of df
        if df[i, 0] == ary_GlobalEventNumber[cidx]:
            df[i, 3] = ary_EventType[cidx]
            df[i, 25] = 1
            cidx += 1
            # end condition
            if cidx >= len(ary_EventType):
                break
        else:
            continue

    # repeat setps with test dataset
    # open to extract NN training predictions and event number
    root_nn_data = uproot.open(root_nn_test)
    root_nn_data_tree = root_nn_data[b"ConeList"]

    # grab entries from leaves "GlobalEventNumber" and "EventType" and store them in arrays
    ary_GlobalEventNumber = root_nn_data_tree["GlobalEventNumber"].array()
    ary_EventType = root_nn_data_tree["EventType"].array()

    # sort both arrays by GlobalEventNumber
    ary_EventType = ary_EventType[ary_GlobalEventNumber.argsort()]
    ary_GlobalEventNumber = ary_GlobalEventNumber[ary_GlobalEventNumber.argsort()]

    # iterate the dataframe and scan for matching entries
    # an extremely dumb iteration algorithm but it works
    cidx = 0
    for i in range(df.shape[0]):
        # start at first entry of df
        if df[i, 0] == ary_GlobalEventNumber[cidx]:
            df[i, 3] = ary_EventType[cidx]
            df[i, 25] = 2
            cidx += 1
            # end condition
            if cidx >= len(ary_EventType):
                break
        else:
            continue

    # export dataframe to compressed .npz

    with open(dir_main + "/data/" + npz_filename, 'wb') as file:
        np.savez_compressed(file, MC_TRUTH=df)

    print("file saved as ", npz_filename)


#################################################################################################################

def generate_npz_data(root_file, npz_filename):
    """
    generate training data based on awals NN preprocessing

    root_file: path to root file containing training data to be generated
    npz_filename: filename for exported .npz file
    """

    from sificc_lib_awal.Simulation import Simulation
    from sificc_lib_awal.DataModel import DataModel

    # load root data into simulation object
    simulation = Simulation(root_file)
    DataModel.generate_training_data(simulation=simulation, output_name=dir_main + "/data/" + npz_filename)


def npz_train_test_split_awal(filename, r):
    """
    take a npz file and split it into training and testing sample

    filename: path to .npz file
    r: train test ratio (percentage of data towards training set)
    """
    import numpy as np

    # load initial npz file
    data = np.load(filename)

    # load npz features based on awals NN
    data_features = data["features"]
    data_targets = data["targets"]
    data_reco = data["reco"]
    data_sequence = data["sequence"]

    # generate index sequence, shuffle sequence and sample by ratio
    idx = np.linspace(0, data_features.shape[0] - 1, data_features.shape[0], dtype=int)
    np.random.shuffle(idx)
    idx_stop = int(len(idx) * r)
    idx_train = idx[0:idx_stop]
    idx_test = idx[idx_stop + 1:]

    # generate npz files
    print("generating training set")
    with open(filename[:-4] + "_training.npz", 'wb') as f_train:
        np.savez_compressed(f_train,
                            features=data_features[idx_train, :],
                            targets=data_targets[idx_train, :],
                            reco=data_reco[idx_train, :],
                            sequence=data_sequence[idx_train])
    print("generating test set")
    with open(filename[:-4] + "_test.npz", 'wb') as f_train:
        np.savez_compressed(f_train,
                            features=data_features[idx_test, :],
                            targets=data_targets[idx_test, :],
                            reco=data_reco[idx_test, :],
                            sequence=data_sequence[idx_test])


#################################################################################################################

generate_npz_resultsMC(root4, root5, root6, "optimized_5mm_MCTRUTH.npz")
# generate_npz_data(root3, "optimized_5mm")
# npz_train_test_split_awal(dir_main + "/data/" + "optimized_0mm.npz", 0.8)
